{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8786d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0 - Policy: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 5.036224003101526\n",
      "Day 1 - Policy: [1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 1.0056176528590794\n",
      "Day 2 - Policy: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 2.0\n",
      "Day 3 - Policy: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 0.0\n",
      "Day 4 - Policy: [1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 7.007965125420448\n",
      "Day 5 - Policy: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 2.0\n",
      "Day 6 - Policy: [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 4.0\n",
      "Day 7 - Policy: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 4.0\n",
      "Day 8 - Policy: [1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 5.0\n",
      "Day 9 - Policy: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], Approximate Expected Cost for Next Day: 4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "NUM_ITEMS = 10\n",
    "NUM_ACTIONS = 11\n",
    "NUM_ACTIONS_P = 6\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "NUM_STATES = NUM_ITEMS + 1  # Add one state for being out of stock\n",
    "\n",
    "# Feature function for state approximation\n",
    "def feature_function(state):\n",
    "    return np.array([state])\n",
    "\n",
    "# Linear function approximation for the value function\n",
    "def approximate_value(theta, state):\n",
    "    return np.dot(theta, feature_function(state))\n",
    "\n",
    "# Adjust the reward function to minimize costs\n",
    "def get_reward(state, action, next_state):\n",
    "    restocking_cost = action\n",
    "    #random.seed(20)\n",
    "    sold_items = np.random.randint(0, 5)  # Simulate a random number of items sold between 0 and 5\n",
    "    lost_sales_cost = max(0, sold_items - state) * 2  # Cost of lost sales due to stockouts\n",
    "    return (restocking_cost + lost_sales_cost)  # We want to minimize costs\n",
    "\n",
    "# Adjust the transition function to reflect the state changes correctly\n",
    "def get_next_state(state, action):\n",
    "    #random.seed(21)\n",
    "    sold_items = random.randint(0, 5)\n",
    "    next_state = max(0, state - sold_items + action)\n",
    "    return next_state\n",
    "\n",
    "# Function to calculate transition probabilities based on a uniform distribution\n",
    "def calculate_transition_probabilities():\n",
    "    probabilities = [1/6] * 6  # Probability of selling 0 to 5 items, equally likely\n",
    "    return probabilities\n",
    "\n",
    "def approximate(theta, state, action, sales_probabilities, gamma):\n",
    "    next_state = get_next_state(state, action)\n",
    "    reward = get_reward(state, action)\n",
    "    expected_cost = sum([sales_probabilities[s] * approximate_value(theta, get_next_state(state, action)) for s in range(NUM_ACTIONS)])\n",
    "    return reward + gamma * expected_cost\n",
    "\n",
    "# Update parameter vector theta\n",
    "def update_theta(theta, state, action, sales_probabilities, gamma, learning_rate):\n",
    "    next_state = get_next_state(state, action)\n",
    "    reward = get_reward(state, action, next_state)\n",
    "    expected_cost = sum([sales_probabilities[s] * approximate_value(theta, get_next_state(state, action)) for s in range(NUM_ACTIONS_P)])\n",
    "    error = reward + gamma * expected_cost - approximate_value(theta, state)\n",
    "    theta += learning_rate * error * feature_function(state)\n",
    "    return theta\n",
    "\n",
    "# Approximate Dynamic Programming\n",
    "def approximate_dynamic_programming():\n",
    "    theta = np.random.rand(feature_function(0).shape[0])  # Initialize weights randomly\n",
    "    policy = [0] * NUM_STATES  # Initialize policy\n",
    "    learning_rate = 0.01  # Learning rate for theta update\n",
    "\n",
    "    # Iterate for each day\n",
    "    for day in range(10):\n",
    "        # Compute the optimal policy for the current day\n",
    "        for state in range(NUM_STATES):\n",
    "            best_action = None\n",
    "            min_expected_cost = float('inf')\n",
    "\n",
    "            # Iterate over actions and find the best action\n",
    "            for action in range(NUM_ACTIONS):\n",
    "                sales_probabilities = [1/6] * 6  # Probability of selling 0 to 5 items, equally likely\n",
    "\n",
    "                expected_cost = get_reward(state, action, next_state) + DISCOUNT_FACTOR * sum([sales_probabilities[s] * approximate_value(theta, next_state)for s in range(NUM_ACTIONS_P)])\n",
    "\n",
    "                if expected_cost < min_expected_cost:\n",
    "                    min_expected_cost = expected_cost\n",
    "                    best_action = action\n",
    "\n",
    "            # Update the policy for the current state\n",
    "            policy[state] = best_action\n",
    "\n",
    "            # Update theta (parameter vector) based on the TD error\n",
    "            theta = update_theta(theta, state, best_action, sales_probabilities, DISCOUNT_FACTOR, learning_rate)\n",
    "            state = next_state\n",
    "            \n",
    "        # Calculate an approximate expected cost for the next day (for illustration purposes)\n",
    "        # For simplicity, we'll use the expected cost for the first state\n",
    "        expected_cost_next_day = get_reward(0, policy[0], get_next_state(0, policy[0])) + DISCOUNT_FACTOR * sum([sales_probabilities[s] * approximate_value(theta, get_next_state(0, policy[0])) for s in range(NUM_ACTIONS_P)])\n",
    "        print(f\"Day {day} - Policy: {policy}, Approximate Expected Cost for Next Day: {expected_cost_next_day}\")\n",
    "\n",
    "# Run the approximate dynamic programming\n",
    "approximate_dynamic_programming()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "93708e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table after learning:\n",
      " [[ 0.          1.28905     0.5160145   0.9         0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.936       0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          1.78767     0.        ]\n",
      " [ 0.4         0.          0.30684047  0.57259049  0.          0.\n",
      "   0.70684047  0.          0.806561    0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.67259049  0.          0.          0.\n",
      "   0.          0.          0.          0.          3.75489344]\n",
      " [ 0.13820049  0.          0.35874347  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.118       0.          0.          0.          1.159369\n",
      "   0.          0.          5.33138658  1.0871163   0.        ]\n",
      " [ 0.2         0.1         0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.59058841  0.4218157   3.65295381  2.02032624  3.2009938   2.86777406\n",
      "   5.54972173 18.73876412  3.02969325  3.8962479   7.61314445]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q-learning\n",
    "# Constants\n",
    "NUM_DAYS = 10  # Horizon\n",
    "NUM_ACTIONS = 11  # Actions: 0 to 9 (restock 0 to 9 items)\n",
    "MAX_ITEMS = 10 # Maximum number of items \n",
    "STATE= 11\n",
    "\n",
    "# Q-Learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.7  # Exploration rate\n",
    "\n",
    "# Q-table initialization\n",
    "Q = np.zeros((11, NUM_ACTIONS))\n",
    "\n",
    "# Function to get the reward for a given state, action, and next state\n",
    "def get_reward(state, action, next_state):\n",
    "    restocking_cost = action\n",
    "    sold_items = np.random.randint(0, 6)  # Simulate a random number of items sold between 0 and 5\n",
    "    lost_sales_cost = max(0, sold_items - next_state) * 2  # Cost of lost sales due to stockouts\n",
    "    return restocking_cost + lost_sales_cost  # We want to minimize costs\n",
    "\n",
    "    \n",
    "# Function to get the next state based on the current state and action\n",
    "def get_next_state(state, action):\n",
    "    sold_items = random.randint(0, 5)\n",
    "    next_state = max(0, state - sold_items + action)  # Update state based on sales\n",
    "    next_state = min(next_state, MAX_ITEMS)  # Update state based on restocking; t\n",
    "    return next_state\n",
    "\n",
    "# Function to choose an action using epsilon-greedy strategy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, NUM_ACTIONS - 1)  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])  # Greedy action\n",
    "\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for day in range(NUM_DAYS):\n",
    "    state = MAX_ITEMS -1  # Start with a full warehouse - 1\n",
    "    \n",
    "    for _ in range(MAX_ITEMS):\n",
    "        # Choose an action\n",
    "        action = choose_action(state)\n",
    "        \n",
    "        # Perform the action and get the reward and new state\n",
    "        next_state = get_next_state(state, action)\n",
    "        \n",
    "        # Ensure next_state is within the valid range\n",
    "        next_state = min(next_state, MAX_ITEMS - 1)\n",
    "        \n",
    "        # Get the reward for the transition\n",
    "        reward = get_reward(state, action, next_state)\n",
    "        \n",
    "        # Update the Q-value for the current state-action pair\n",
    "        old_value = Q[state, action]\n",
    "        next_max = np.max(Q[next_state, :])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        Q[state, action] = new_value\n",
    "        \n",
    "        # Move to the new state\n",
    "        state = next_state\n",
    "\n",
    "# Print the Q-table\n",
    "print(\"Q-table after learning:\\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec43936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
