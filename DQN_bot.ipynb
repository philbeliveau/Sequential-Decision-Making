{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en6KJXSVJtQa"
      },
      "outputs": [],
      "source": [
        "# DQN\n",
        "\n",
        "\n",
        "\n",
        "import codey, event, time, rocky, random, ujson, urequests, json\n",
        "import time, random\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# Class definition for Deep Q Network (DQN)\n",
        "class DQN:\n",
        "    \"\"\"\n",
        "        Initialize the DQN object with the following parameters:\n",
        "        - state_dim: The dimensionality of the state space.\n",
        "        - action_dim: The dimensionality of the action space.\n",
        "        - hidden_dims: A list containing the number of neurons in each hidden layer.\n",
        "        - lr: The learning rate for the DQN.\n",
        "        - gamma: The discount factor for future rewards.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, hidden_dims, lr=0.05, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_layers = len(hidden_dims)\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Neural Network\n",
        "        self.weights = []\n",
        "        previous_dim = state_dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            self.weights.append([[random.uniform(-1, 1) for _ in range(hidden_dim)] for _ in range(previous_dim)])\n",
        "            previous_dim = hidden_dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Final layer should have dimensionality matching action space\n",
        "        self.weights.append([[random.uniform(-1, 1) for _ in range(action_dim)] for _ in range(previous_dim)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through the network. Accepts a state as input and returns the action values as output.\n",
        "        \"\"\"\n",
        "        self.a = []\n",
        "        self.z = []\n",
        "\n",
        "        if isinstance(state, int):\n",
        "            state_list = [0]*self.state_dim\n",
        "            state_list[state] = 1\n",
        "            state = state_list\n",
        "\n",
        "        if not isinstance(state, list):  # Ensure state is always a list\n",
        "            state = [state]\n",
        "\n",
        "        self.a.append(state)\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.z.append(self.dot(self.a[-1], self.weights[i]))\n",
        "            self.a.append([self.relu(x) for x in self.z[-1]])\n",
        "\n",
        "        return self.a[-1][0:self.action_dim]\n",
        "\n",
        "    def backward(self, state, action, target):\n",
        "        \"\"\"\n",
        "        Backward pass through the network. Accepts a state, action, and target as inputs and updates the weights of the network.\n",
        "        \"\"\"\n",
        "        if not isinstance(state, list):  # Ensure state is always a list\n",
        "            state = [state]\n",
        "\n",
        "        self.a = [state, ] + self.a\n",
        "        deltas = [[0]*len(layer) for layer in self.weights]  # Initialize deltas\n",
        "\n",
        "        # Compute output layer delta\n",
        "        deltas[-1] = [0]*len(self.weights[-1])  # Initialize last layer delta\n",
        "        deltas[-1][action] = target - self.a[-1][action]\n",
        "\n",
        "        # Backpropagate deltas\n",
        "        for i in reversed(range(len(self.weights) - 1)):  # Start from the end, excluding the last layer\n",
        "            for k in range(len(self.weights[i+1])):  # For each neuron in the next layer\n",
        "                for j in range(len(self.weights[i+1][k])):  # For each neuron in the current layer\n",
        "                    if j < len(self.a[i+1]):\n",
        "                        deltas[i][j] += deltas[i+1][k] * self.weights[i+1][k][j] * (self.a[i+1][j] > 0)  # Apply ReLU derivative\n",
        "\n",
        "        # Update weights\n",
        "        for i in range(len(self.weights)):\n",
        "            for j in range(len(self.weights[i])):\n",
        "                for k in range(len(self.weights[i][j])):\n",
        "                    if j < len(self.a[i]) and k < len(deltas[i]):\n",
        "                        self.weights[i][j][k] += self.lr * self.a[i][j] * deltas[i][k]  # Update weights using the deltas\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Update the network given a state, action, reward, and the next state.\n",
        "        \"\"\"\n",
        "        forward_state = self.forward(state)\n",
        "        future_rewards = max(self.forward(next_state))\n",
        "        target = reward + self.lr * (reward + self.gamma * future_rewards - forward_state[action])\n",
        "        self.backward(state, action, target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        Select an action for a given state using epsilon-greedy action selection.\n",
        "        \"\"\"\n",
        "        forward_state = self.forward(state)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            return random.randint(0, self.action_dim-1)\n",
        "        else:\n",
        "            return forward_state.index(max(forward_state))\n",
        "\n",
        "\n",
        "    def dot(self, a, b):\n",
        "        \"\"\"\n",
        "        Dot product operation.\n",
        "        \"\"\"\n",
        "        # If 'a' and 'b' are 1D lists\n",
        "        if not any(isinstance(i, list) for i in a) and not any(isinstance(i, list) for i in b):\n",
        "                return sum(x*y for x, y in zip(a, b))\n",
        "\n",
        "        # If 'a' and 'b' are 2D lists\n",
        "        elif all(isinstance(i, list) for i in a) and all(isinstance(i, list) for i in b):\n",
        "            b_t = list(map(list, zip(*b))) # Transpose b\n",
        "            return [[sum(x*y for x, y in zip(row_a, row_b)) for row_b in b_t] for row_a in a]\n",
        "\n",
        "        # 'b' is a 2D list, 'a' is a 1D list\n",
        "        elif all(isinstance(i, list) for i in b):\n",
        "            return [sum(x*y for x, y in zip(a, neuron_weights)) for neuron_weights in b]\n",
        "\n",
        "        # 'b' is a 1D list, 'a' is a 1D list\n",
        "        else:\n",
        "            return sum(x*y for x, y in zip(a, b))\n",
        "\n",
        "    def relu(self, x):\n",
        "        \"\"\"\n",
        "        ReLU (Rectified Linear Unit) activation function.\n",
        "        \"\"\"\n",
        "        return max(0, x)\n",
        "\n",
        "    def outer(self, a, b):\n",
        "        \"\"\"\n",
        "        Outer product operation.\n",
        "        \"\"\"\n",
        "        if isinstance(b, float):\n",
        "            return [[x*b for _ in range(len(a))] for x in a]\n",
        "        else:\n",
        "            return [[x*y for y in b] for x in a]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Sigmoid activation function.\n",
        "        \"\"\"\n",
        "        return [1 / (1 + math.exp(-xi)) for xi in x]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a pseudo-environment\n",
        "class PseudoEnvironment:\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the PseudoEnvironment object.\n",
        "        \"\"\"\n",
        "        self.current_state = 0 # Start with state 1\n",
        "        self.last_transition_time = codey.get_timer()\n",
        "\n",
        "\n",
        "\n",
        "    def get_color(self):\n",
        "        if rocky.color_ir_sensor.is_color('black'):\n",
        "            self.current_state = 0\n",
        "        if rocky.color_ir_sensor.is_color('green'):\n",
        "            self.current_state = 1\n",
        "        if rocky.color_ir_sensor.is_color('white'):\n",
        "            self.current_state = 2\n",
        "        if rocky.color_ir_sensor.is_color('blue'):\n",
        "            self.current_state = 3\n",
        "        return self.current_state\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0  # Default reward\n",
        "\n",
        "\n",
        "\n",
        "        if self.current_state == 0:\n",
        "            if rocky.color_ir_sensor.is_color('black'):\n",
        "                rocky.drive(50, 0)\n",
        "            else:\n",
        "                rocky.drive(0, 50)\n",
        "\n",
        "\n",
        "\n",
        "        if self.current_state == 1:\n",
        "            if action == 1:\n",
        "                rocky.turn_left_by_degree(90)\n",
        "                rocky.forward(20, 1)\n",
        "                while self.current_state == 1:\n",
        "                    if rocky.color_ir_sensor.is_color('black'):\n",
        "                        rocky.drive(0, 40)\n",
        "                    else:\n",
        "                        rocky.drive(40, 0)\n",
        "                    if rocky.color_ir_sensor.is_color('white'):\n",
        "                        break\n",
        "            else:\n",
        "                rocky.turn_right_by_degree(90)\n",
        "                rocky.forward(20, 1)\n",
        "                while self.current_state == 1:\n",
        "                    if rocky.color_ir_sensor.is_color('black'):\n",
        "                        rocky.drive(40, 0)\n",
        "                    else:\n",
        "                        rocky.drive(0, 40)\n",
        "                    if rocky.color_ir_sensor.is_color('white'):\n",
        "                        break\n",
        "            current_time = codey.get_timer()\n",
        "            time_elapsed = current_time - self.last_transition_time\n",
        "            self.last_transition_time = codey.get_timer()\n",
        "\n",
        "\n",
        "\n",
        "            reward =  1 - 0.1 * time_elapsed\n",
        "        elif self.current_state == 2:\n",
        "            if action == 1:\n",
        "                rocky.turn_left_by_degree(90)\n",
        "                rocky.forward(20, 1)\n",
        "                while self.current_state == 2:\n",
        "                    if rocky.color_ir_sensor.is_color('black'):\n",
        "                        rocky.drive(0, 40)\n",
        "                    else:\n",
        "                        rocky.drive(40, 0)\n",
        "                    if rocky.color_ir_sensor.is_color('blue'):\n",
        "                        break\n",
        "            else:\n",
        "                rocky.turn_right_by_degree(90)\n",
        "                rocky.forward(20, 0.5)\n",
        "                while self.current_state == 2:\n",
        "                    if rocky.color_ir_sensor.is_color('black'):\n",
        "                        rocky.drive(30, 0)\n",
        "                    else:\n",
        "                        rocky.drive(0, 30)\n",
        "                    if rocky.color_ir_sensor.is_color('blue'):\n",
        "                        break\n",
        "\n",
        "\n",
        "\n",
        "            current_time = codey.get_timer()\n",
        "            time_elapsed = current_time - self.last_transition_time\n",
        "            self.last_transition_time = codey.get_timer()\n",
        "            reward =  1 - 0.1 * time_elapsed\n",
        "\n",
        "\n",
        "\n",
        "        #     reward = 1 - 0.1 * time_elapsed\n",
        "        elif self.current_state == 3:\n",
        "\n",
        "\n",
        "\n",
        "            current_time = codey.get_timer()\n",
        "            codey.display.show(current_time)\n",
        "            time_elapsed = current_time - self.last_transition_time\n",
        "            self.last_transition_time = codey.get_timer()\n",
        "            reward = 100 + (1 - 0.1 * time_elapsed)\n",
        "            rocky.stop()\n",
        "\n",
        "\n",
        "\n",
        "        next_state = self.current_state + 1\n",
        "        if (next_state > 3):\n",
        "            next_state = 0\n",
        "\n",
        "\n",
        "\n",
        "        return next_state, reward, current_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training loop for Q-Learning\n",
        "@event.button_a_pressed\n",
        "def on_button_a_pressed():\n",
        "    global action, dqn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize DQN, Q-Learning and Environment\n",
        "    dqn = DQN(state_dim=4, action_dim=2, hidden_dims=[10,10], lr=0.5, gamma=0.95)\n",
        "    env = PseudoEnvironment()\n",
        "\n",
        "    # Initialize lists to store total rewards per episode\n",
        "    rewards_dqn = [ ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Number of episodes for Q-Learning\n",
        "    num_episodes = 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        codey.display.show('E:'+str(i))\n",
        "        total_reward = 0\n",
        "        codey.reset_timer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        while(1):\n",
        "            state = env.get_color() # 1\n",
        "            codey.display.show(state)\n",
        "            if state != 0:\n",
        "                action = dqn.get_action(state, epsilon=0.5)\n",
        "                next_state, reward, current_time = env.step(action)\n",
        "                #codey.display.show(\"N:\"+str(next_state))\n",
        "                dqn.update(state, action, reward, next_state)\n",
        "                total_reward += reward\n",
        "                rewards_dqn.append(total_reward)\n",
        "\n",
        "                while(env.get_color() != next_state):\n",
        "                    #codey.display.show(str(env.get_color())+'->'+str(next_state))\n",
        "                    continue\n",
        "                if next_state == 0:\n",
        "                    break\n",
        "\n",
        "            else:\n",
        "                rocky.drive(5,5)\n",
        "                while(state == env.get_color()):\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        c_time = current_time\n",
        "        print(rewards_dqn)\n",
        "        print(c_time)"
      ]
    }
  ]
}